{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer_dims,init_type='he_normal',seed=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "                  layer_dims lis is like  [ no of input features,# of neurons in hidden layer-1,..,\n",
    "                                     # of neurons in hidden layer-n shape,output]\n",
    "    init_type -- he_normal  --> N(0,sqrt(2/fanin)) used in case of ReLu to offset non-zero mean\n",
    "                 he_uniform --> Uniform(-sqrt(6/fanin),sqrt(6/fanin))\n",
    "                 xavier_normal --> N(0,2/(fanin+fanout))\n",
    "                 xavier_uniform --> Uniform(-sqrt(6/fanin+fanout),sqrt(6/fanin+fanout))\n",
    "    seed -- random seed to generate weights\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "    if  init_type == 'he_normal':\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n",
    "            parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], 1))\n",
    "\n",
    "    elif init_type == 'he_uniform':\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "            parameters['b' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], 1))\n",
    "\n",
    "    elif init_type == 'xavier_normal':\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n",
    "            parameters['b' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),(layer_dims[l], 1))\n",
    "\n",
    "    elif init_type == 'xavier_uniform':\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "            parameters['b' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def xavier_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        else:\n",
    "            bound = math.sqrt(6) / math.sqrt(param.shape[0] + param.shape[1])\n",
    "            param.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "xavier_init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        elif name.startswith(\"layers.0\"):  # The first layer does not have ReLU applied on its input\n",
    "            param.data.normal_(0, 1 / math.sqrt(param.shape[1]))\n",
    "        else:\n",
    "            param.data.normal_(0, math.sqrt(2) / math.sqrt(param.shape[1]))\n",
    "\n",
    "\n",
    "model = BaseNetwork(act_fn=nn.ReLU())\n",
    "kaiming_init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class BaseNetwork(torch.nn.Module):\n",
    "    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n",
    "        \"\"\"Base Network.\n",
    "\n",
    "        Args:\n",
    "            act_fn: Object of the activation function that should be used as non-linearity in the network.\n",
    "            input_size: Size of the input images in pixels\n",
    "            num_classes: Number of classes we want to predict\n",
    "            hidden_sizes: A list of integers specifying the hidden layer sizes in the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the network based on the specified hidden sizes\n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        for layer_index in range(1, len(layer_sizes)):\n",
    "            layers += [nn.Linear(layer_sizes[layer_index - 1], layer_sizes[layer_index]), act_fn]\n",
    "        layers += [nn.Linear(layer_sizes[-1], num_classes)]\n",
    "        # A module list registers a list of modules as submodules (e.g. for parameters)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.config = {\n",
    "            \"act_fn\": act_fn.__class__.__name__,\n",
    "            \"input_size\": input_size,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"hidden_sizes\": hidden_sizes,\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "class Identity(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "model = BaseNetwork(act_fn=Identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
